{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source](https://pub.towardsai.net/big-data-pipelines-with-sparkml-8207c86fc995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('HMP') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Titanic Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc302523be0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "import os\n",
    "\n",
    "schema = StructType([\n",
    "                     StructField('x',IntegerType(),True),\n",
    "                     StructField('y',IntegerType(),True),\n",
    "                     StructField('z',IntegerType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eat_soup',\n",
       " 'Descend_stairs',\n",
       " 'Getup_bed',\n",
       " 'Climb_stairs',\n",
       " 'Sitdown_chair',\n",
       " 'Standup_chair',\n",
       " 'Eat_meat',\n",
       " 'MANUAL.txt',\n",
       " 'final.py',\n",
       " 'impdata.py',\n",
       " 'README.txt',\n",
       " 'Use_telephone',\n",
       " 'Drink_glass',\n",
       " 'Brush_teeth',\n",
       " 'Pour_water',\n",
       " 'Comb_hair',\n",
       " '.git',\n",
       " 'Liedown_bed',\n",
       " 'Walk',\n",
       " '.idea']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = os.listdir('data/HMP_Dataset')\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eat_soup',\n",
       " 'Descend_stairs',\n",
       " 'Getup_bed',\n",
       " 'Climb_stairs',\n",
       " 'Sitdown_chair',\n",
       " 'Standup_chair',\n",
       " 'Eat_meat',\n",
       " 'Use_telephone',\n",
       " 'Drink_glass',\n",
       " 'Brush_teeth',\n",
       " 'Pour_water',\n",
       " 'Comb_hair',\n",
       " 'Liedown_bed',\n",
       " 'Walk']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s remove non-action folders from the file list.\n",
    "# These are typically folders without an underscore in their names.\n",
    "\n",
    "file_list_filtered = [i for i in file_list if '_' in i or i == 'Walk']\n",
    "file_list_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:41<00:00,  2.94s/it]\n"
     ]
    }
   ],
   "source": [
    "# Okay, we have all the folders containing the data in one array. Now we can iterate over this array.\n",
    "# First we define an empty data frame that we'd append data to\n",
    "df = None\n",
    "# next we import tqdm progress bars to see how our code runs \n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "# The lit library helps us write string literals column to an apache dataframe.\n",
    "\n",
    "# Now let's iterate through the folders\n",
    "for category in tqdm(file_list_filtered):\n",
    "    # Now we traverse all through the files in each folder\n",
    "    data_files = os.listdir('data/HMP_Dataset/' + category)\n",
    "    for data_file in data_files:\n",
    "        # Now we create a temporary dataframe\n",
    "        # we use our defined schema above\n",
    "        temp_df = spark.read.option('header','false').option('delimiter',' ').csv('data/HMP_Dataset/'+ category + '/' + data_file, schema=schema)  \n",
    "        temp_df = temp_df.withColumn('class',lit(category))  # Adding a class column to the dataframe\n",
    "        temp_df = temp_df.withColumn('source',lit(data_file))  # Adding a source column to the dataframe\n",
    "        # now we put a condition if df is empty\n",
    "        if df is None:\n",
    "            df = temp_df\n",
    "        else:\n",
    "            df = df.union(temp_df)  # else union appends the data frames vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      " |-- z: integer (nullable = true)\n",
      " |-- class: string (nullable = false)\n",
      " |-- source: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+--------------------+\n",
      "|  x|  y|  z|   class|              source|\n",
      "+---+---+---+--------+--------------------+\n",
      "| 36| 37| 51|Eat_soup|Accelerometer-201...|\n",
      "| 36| 37| 51|Eat_soup|Accelerometer-201...|\n",
      "| 35| 38| 53|Eat_soup|Accelerometer-201...|\n",
      "| 36| 39| 52|Eat_soup|Accelerometer-201...|\n",
      "| 36| 38| 51|Eat_soup|Accelerometer-201...|\n",
      "| 35| 37| 51|Eat_soup|Accelerometer-201...|\n",
      "| 36| 38| 52|Eat_soup|Accelerometer-201...|\n",
      "| 36| 37| 52|Eat_soup|Accelerometer-201...|\n",
      "| 36| 38| 51|Eat_soup|Accelerometer-201...|\n",
      "| 37| 38| 51|Eat_soup|Accelerometer-201...|\n",
      "| 35| 38| 51|Eat_soup|Accelerometer-201...|\n",
      "| 36| 38| 52|Eat_soup|Accelerometer-201...|\n",
      "| 36| 38| 52|Eat_soup|Accelerometer-201...|\n",
      "| 37| 39| 51|Eat_soup|Accelerometer-201...|\n",
      "| 36| 39| 51|Eat_soup|Accelerometer-201...|\n",
      "| 35| 38| 52|Eat_soup|Accelerometer-201...|\n",
      "| 34| 38| 52|Eat_soup|Accelerometer-201...|\n",
      "| 35| 39| 52|Eat_soup|Accelerometer-201...|\n",
      "| 35| 38| 52|Eat_soup|Accelerometer-201...|\n",
      "| 37| 38| 52|Eat_soup|Accelerometer-201...|\n",
      "+---+---+---+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+--------------------+----------+\n",
      "|  x|  y|  z|   class|              source|classIndex|\n",
      "+---+---+---+--------+--------------------+----------+\n",
      "| 36| 37| 51|Eat_soup|Accelerometer-201...|      13.0|\n",
      "| 36| 37| 51|Eat_soup|Accelerometer-201...|      13.0|\n",
      "| 35| 38| 53|Eat_soup|Accelerometer-201...|      13.0|\n",
      "| 36| 39| 52|Eat_soup|Accelerometer-201...|      13.0|\n",
      "| 36| 38| 51|Eat_soup|Accelerometer-201...|      13.0|\n",
      "| 35| 37| 51|Eat_soup|Accelerometer-201...|      13.0|\n",
      "| 36| 38| 52|Eat_soup|Accelerometer-201...|      13.0|\n",
      "| 36| 37| 52|Eat_soup|Accelerometer-201...|      13.0|\n",
      "| 36| 38| 51|Eat_soup|Accelerometer-201...|      13.0|\n",
      "| 37| 38| 51|Eat_soup|Accelerometer-201...|      13.0|\n",
      "+---+---+---+--------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol = 'class', outputCol = 'classIndex')\n",
    "indexed = indexer.fit(df).transform(df)  # This is a new data frame\n",
    "\n",
    "# Let's see it\n",
    "indexed.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+-------------------------------------------------+----------+---------------+\n",
      "|x  |y  |z  |class   |source                                           |classIndex|categoryVec    |\n",
      "+---+---+---+--------+-------------------------------------------------+----------+---------------+\n",
      "|36 |37 |51 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|\n",
      "|36 |37 |51 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|\n",
      "|35 |38 |53 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|\n",
      "|36 |39 |52 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|\n",
      "|36 |38 |51 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|\n",
      "|35 |37 |51 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|\n",
      "|36 |38 |52 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|\n",
      "|36 |37 |52 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|\n",
      "|36 |38 |51 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|\n",
      "|37 |38 |51 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|\n",
      "+---+---+---+--------+-------------------------------------------------+----------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# The OneHotEncoder is a pure transformer object. it does not use the fit()\n",
    "encoder = OneHotEncoder(inputCol = 'classIndex', outputCol = 'categoryVec')\n",
    "encoder.setDropLast(False)\n",
    "ohe = encoder.fit(indexed)  # This is a new data frame\n",
    "encoded = ohe.transform(indexed)  # This is a new data frame\n",
    "\n",
    "encoded.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+-------------------------------------------------+----------+---------------+----------------+\n",
      "|x  |y  |z  |class   |source                                           |classIndex|categoryVec    |features        |\n",
      "+---+---+---+--------+-------------------------------------------------+----------+---------------+----------------+\n",
      "|36 |37 |51 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|[36.0,37.0,51.0]|\n",
      "|36 |37 |51 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|[36.0,37.0,51.0]|\n",
      "|35 |38 |53 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|[35.0,38.0,53.0]|\n",
      "|36 |39 |52 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|[36.0,39.0,52.0]|\n",
      "|36 |38 |51 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|[36.0,38.0,51.0]|\n",
      "|35 |37 |51 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|[35.0,37.0,51.0]|\n",
      "|36 |38 |52 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|[36.0,38.0,52.0]|\n",
      "|36 |37 |52 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|[36.0,37.0,52.0]|\n",
      "|36 |38 |51 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|[36.0,38.0,51.0]|\n",
      "|37 |38 |51 |Eat_soup|Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt|13.0      |(14,[13],[1.0])|[37.0,38.0,51.0]|\n",
      "+---+---+---+--------+-------------------------------------------------+----------+---------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# VectorAssembler creates vectors from ordinary data types for us\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['x','y','z'], outputCol = 'features')\n",
    "# Now we use the vectorAssembler object to transform our last updated dataframe\n",
    "\n",
    "features_vectorized = vectorAssembler.transform(encoded)  # note this is a new df\n",
    "\n",
    "# Let's see the first 10 rows\n",
    "features_vectorized.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+--------------------+----------+---------------+----------------+--------------------+\n",
      "|  x|  y|  z|   class|              source|classIndex|    categoryVec|        features|       features_norm|\n",
      "+---+---+---+--------+--------------------+----------+---------------+----------------+--------------------+\n",
      "| 36| 37| 51|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,37.0,51.0]|[0.29032258064516...|\n",
      "| 36| 37| 51|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,37.0,51.0]|[0.29032258064516...|\n",
      "| 35| 38| 53|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[35.0,38.0,53.0]|[0.27777777777777...|\n",
      "| 36| 39| 52|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,39.0,52.0]|[0.28346456692913...|\n",
      "| 36| 38| 51|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,38.0,51.0]| [0.288,0.304,0.408]|\n",
      "| 35| 37| 51|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[35.0,37.0,51.0]|[0.28455284552845...|\n",
      "| 36| 38| 52|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,38.0,52.0]|[0.28571428571428...|\n",
      "| 36| 37| 52|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,37.0,52.0]| [0.288,0.296,0.416]|\n",
      "| 36| 38| 51|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,38.0,51.0]| [0.288,0.304,0.408]|\n",
      "| 37| 38| 51|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[37.0,38.0,51.0]|[0.29365079365079...|\n",
      "+---+---+---+--------+--------------------+----------+---------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "normalizer = Normalizer(inputCol = 'features', outputCol = 'features_norm', p=1.0)  # Manhattan Distance\n",
    "normalized_data = normalizer.transform(features_vectorized) # New data frame too.\n",
    "\n",
    "normalized_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = [indexer,encoder,vectorAssembler,normalizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+--------------------+----------+---------------+----------------+--------------------+\n",
      "|  x|  y|  z|   class|              source|classIndex|    categoryVec|        features|       features_norm|\n",
      "+---+---+---+--------+--------------------+----------+---------------+----------------+--------------------+\n",
      "| 36| 37| 51|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,37.0,51.0]|[0.29032258064516...|\n",
      "| 36| 37| 51|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,37.0,51.0]|[0.29032258064516...|\n",
      "| 35| 38| 53|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[35.0,38.0,53.0]|[0.27777777777777...|\n",
      "| 36| 39| 52|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,39.0,52.0]|[0.28346456692913...|\n",
      "| 36| 38| 51|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,38.0,51.0]| [0.288,0.304,0.408]|\n",
      "| 35| 37| 51|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[35.0,37.0,51.0]|[0.28455284552845...|\n",
      "| 36| 38| 52|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,38.0,52.0]|[0.28571428571428...|\n",
      "| 36| 37| 52|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,37.0,52.0]| [0.288,0.296,0.416]|\n",
      "| 36| 38| 51|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[36.0,38.0,51.0]| [0.288,0.304,0.408]|\n",
      "| 37| 38| 51|Eat_soup|Accelerometer-201...|      13.0|(14,[13],[1.0])|[37.0,38.0,51.0]|[0.29365079365079...|\n",
      "+---+---+---+--------+--------------------+----------+---------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_model = pipeline.fit(df)\n",
    "pipelined_data = data_model.transform(df)\n",
    "pipelined_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|    categoryVec|       features_norm|\n",
      "+---------------+--------------------+\n",
      "|(14,[13],[1.0])|[0.29032258064516...|\n",
      "|(14,[13],[1.0])|[0.29032258064516...|\n",
      "|(14,[13],[1.0])|[0.27777777777777...|\n",
      "|(14,[13],[1.0])|[0.28346456692913...|\n",
      "|(14,[13],[1.0])| [0.288,0.304,0.408]|\n",
      "|(14,[13],[1.0])|[0.28455284552845...|\n",
      "|(14,[13],[1.0])|[0.28571428571428...|\n",
      "|(14,[13],[1.0])| [0.288,0.296,0.416]|\n",
      "|(14,[13],[1.0])| [0.288,0.304,0.408]|\n",
      "|(14,[13],[1.0])|[0.29365079365079...|\n",
      "+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first let's list out the columns we want to drop\n",
    "cols_to_drop = ['x','y','z','class','source','classIndex','features']\n",
    "\n",
    "# Next let's use a list comprehension with conditionals to select cols we need\n",
    "selected_cols = [col for col in pipelined_data.columns if col not in cols_to_drop]\n",
    "\n",
    "# Let's define a new train_df with only the categoryVec and features_norm cols\n",
    "df_train = pipelined_data.select(selected_cols)\n",
    "\n",
    "# Let's see our training dataframe.\n",
    "df_train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
